version: 2
jobs:
  build:
    environment:
      IMAGE_NAME: guangie88/spark-custom
    docker:
    - image: circleci/python:3-stretch
      environment:
        SPARK_VERSION: 2.4.0
        HADOOP_VERSION: 3.1.0
        WITH_HIVE: "true"
      
    steps:
    - checkout

    - run:
        name: Build Docker image
        command: |
          docker build . -t ${IMAGE_NAME}:ref \
            --build-arg SPARK_VERSION=${SPARK_VERSION} \
            --build-arg HADOOP_VERSION=${HADOOP_VERSION} \
            --build-arg WITH_HIVE=${WITH_HIVE}

  publish:
    docker:
    - image: circleci/python:3-stretch
      environment:
        SPARK_VERSION: 2.4.0
        HADOOP_VERSION: 3.1.0
        WITH_HIVE: "true"

    steps:
    - run:
        name: Log in to Docker
        command: docker login -u ${DOCKER_USERNAME} -p ${DOCKER_PASSWORD}

    - run:
        name: Push image into Docker
        command: |
          HIVE_TAG_SUFFIX=$(if [ "${WITH_HIVE}" = "true" ]; then echo _with-hive; fi)
          TAG_NAME=${SPARK_VERSION}_hadoop-${HADOOP_VERSION}${HIVE_TAG_SUFFIX}
          docker tag ${IMAGE_NAME}:ref ${IMAGE_NAME}:${TAG_NAME}
          docker push ${IMAGE_NAME}:${TAG_NAME}

workflows:
  version: 2
  build_and_publish:
    jobs:
    - build
    - publish:
        requires:
          - build
        # filters:
        #   branches:
        #     only:
        #     - master
