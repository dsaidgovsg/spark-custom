version: 2

#
# Build step
#

shared_build: &shared_build
  environment:
    IMAGE_NAME: guangie88/spark-custom
    
  steps:
  - checkout
  - setup_remote_docker:
      docker_layer_caching: true
  - run:
      name: Get additional values for checking cache
      command: |
        DOCKER_VERSION=$(docker --version)
        echo ${DOCKER_VERSION} >> /tmp/cache-hash

        DOCKER_BUILD_IMAGE_DIGEST="$(docker pull ${BUILDER_IMAGE} | grep 'Digest:' | sed -E 's/Digest: (.+)/\1/')"
        echo ${DOCKER_BUILD_IMAGE_DIGEST} >> /tmp/cache-hash

        DOCKER_RELEASE_IMAGE_DIGEST="$(docker pull ${RELEASE_IMAGE} | grep 'Digest:' | sed -E 's/Digest: (.+)/\1/')"
        echo ${DOCKER_RELEASE_IMAGE_DIGEST} >> /tmp/cache-hash

        DOCKERFILE_HASH=$(cat Dockerfile-${DIST} | md5sum | cut -d' ' -f1)
        echo ${DOCKERFILE_HASH} >> /tmp/cache-hash

        echo ${SPARK_VERSION} >> /tmp/cache-hash
        echo ${HADOOP_VERSION} >> /tmp/cache-hash
        echo ${WITH_HIVE} >> /tmp/cache-hash
        echo ${WITH_PYSPARK} >> /tmp/cache-hash

        echo ${DIST} >> /tmp/cache-hash

        cat /tmp/cache-hash

  - restore_cache:
      keys:
      - v1-spark-custom-{{ checksum "/tmp/cache-hash" }}
  - run:
      name: Create artifacts directory
      command: mkdir -p /tmp/artifacts
  - run:
      name: Build Docker image and save image tar as artifact
      command: |
        HIVE_TAG_SUFFIX=$(if [ "${WITH_HIVE}" = "true" ]; then echo _with-hive; fi)
        PYSPARK_TAG_SUFFIX=$(if [ "${WITH_PYSPARK}" = "true" ]; then echo _with-pyspark; fi)
        TAG_NAME=${SPARK_VERSION}_hadoop-${HADOOP_VERSION}${HIVE_TAG_SUFFIX}${PYSPARK_TAG_SUFFIX}_${DIST}
        echo "${IMAGE_NAME}:${TAG_NAME}" > /tmp/artifacts/image-tag-name

        if [ ! -f /tmp/artifacts/docker-image.tar ]; then
          docker build . -f Dockerfile-${DIST} -t ${IMAGE_NAME}:ref \
            --build-arg SPARK_VERSION=${SPARK_VERSION} \
            --build-arg HADOOP_VERSION=${HADOOP_VERSION} \
            --build-arg WITH_HIVE=${WITH_HIVE} \
            --build-arg WITH_PYSPARK=${WITH_PYSPARK} \
            ;
          
          docker save ${IMAGE_NAME}:${TAG_NAME} -o /tmp/artifacts/docker-image.tar
        fi
  - save_cache:
      key: v1-spark-custom-{{ checksum "/tmp/cache-hash" }}
      paths:
      - /tmp/artifacts

  - persist_to_workspace:
      root: /tmp/artifacts
      paths:
      - docker-image.tar
      - image-tag-name

#
# Publish step
#

shared_publish: &shared_publish
  docker:
  - image: circleci/python:3-stretch
  steps:
  - attach_workspace:
      at: /tmp/artifacts
  - setup_remote_docker:
      docker_layer_caching: true
  - run:
      name: Log in to Docker registry
      command: docker login -u ${DOCKER_USERNAME} -p ${DOCKER_PASSWORD}
  - run:
      name: Load from previous Docker build and push image into Docker
      command: |
        REF_IMAGE_TAG_NAME=$(docker load -i /tmp/artifacts/docker-image.tar | sed -E 's/Loaded image: (.+)/\1/')
        IMAGE_TAG_NAME=$(cat /tmp/artifacts/image-tag-name)
        docker load -i /tmp/artifacts/docker-image.tar
        docker tag ${REF_IMAGE_TAG_NAME} ${IMAGE_TAG_NAME}
        docker push ${IMAGE_TAG_NAME}

#
# Jobs and workflows
#

jobs:
  debian_build:
    <<: *shared_build
    docker:
    - image: circleci/python:3-stretch
      environment:
        BUILDER_IMAGE: openjdk:8-jdk-slim
        RELEASE_IMAGE: openjdk:8-jre-slim
        SPARK_VERSION: 2.4.0
        HADOOP_VERSION: 3.1.0
        WITH_HIVE: "true"
        WITH_PYSPARK: "true"
        DIST: debian
  alpine_build:
    <<: *shared_build
    docker:
    - image: circleci/python:3-stretch
      environment:
        BUILDER_IMAGE: openjdk:8-jdk-alpine
        RELEASE_IMAGE: openjdk:8-jre-alpine
        SPARK_VERSION: 2.4.0
        HADOOP_VERSION: 3.1.0
        WITH_HIVE: "true"
        WITH_PYSPARK: "true"
        DIST: alpine
  publish:
    <<: *shared_publish

workflows:
  version: 2
  debian_build_and_publish:
    jobs:
    - debian_build
    - publish:
        requires:
          - debian_build
        filters:
          branches:
            only:
            - master
  alpine_build_and_publish:
    jobs:
    - alpine_build
    - publish:
        requires:
          - alpine_build
        filters:
          branches:
            only:
            - master
